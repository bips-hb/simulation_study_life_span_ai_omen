{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XAI Faithfulness Evaluation\n\nMeasures how well IG and EG recover the **three injected ground-truth patterns** from the simulation, across all scenarios.\n\n**Ground-truth patterns:**\n- **Trigger** \u2014 ICD_10 at one visit in 1\u201310, followed by ATC_5 at the next visit. Contribution weight: +10 (dominant signal)\n- **Escalation** \u2014 broad ICD spike across the last 3 visits. Contribution weight: +2.5\n- **Chronic** \u2014 ICD_1, ICD_2, ICD_3 recurring throughout history. Contribution weight: +0.25 per occurrence (weakest)\n\n**Three faithfulness metrics per phenotype per method:**\n- **Spearman r** \u2014 rank correlation between |XAI| and ground-truth attribution (flattened over time \u00d7 features per patient, then averaged)\n- **Top-K feature hit** \u2014 fraction of patients where \u22651 ground-truth feature appears in the top-K features by mean |XAI|\n- **Temporal precision** \u2014 fraction of total |XAI| mass that falls in the visits where the ground-truth signal actually occurs\n\n**Requires:** `faithfulness.csv` in each scenario folder (produced by the pipeline after adding `run_faithfulness_evaluation()`).\nIf not yet run, Section 2 can compute them on the fly from saved `.npy` files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os, glob\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\nimport seaborn as sns\nfrom IPython.display import display\n\nOUT_DIR = \"/home/hoepler/longitudinal_prediction_modeling/pipeline_out\"\nSIM_DIR = \"/mnt/c/Users/hoepler/Documents/LifeSpanAI_OMEN/simulation_study_life_span_ai_omen/sim_outputs\"\n\nSCENARIOS = [\n    \"Baseline\", \"Small N\", \"Large N\",\n    \"Rare Trigger\", \"Very Rare Trigger\",\n    \"Short History\", \"Long History\",\n    \"High Noise\", \"Very High Noise\",\n    \"Stress Test\",\n]\nMODELS  = [\"GRU\", \"LSTM\"]\nMETHODS = [f\"{xai}_{m}\" for xai in [\"EG\",\"IG\"] for m in MODELS]\n\ndef sc_dir(name):\n    return os.path.join(OUT_DIR, name.replace(\" \", \"_\"))\n\n# Ground-truth feature indices (0-based, in sorted feature_names list)\nGT = {\n    \"TRIGGER_ICD\": 41,   # ICD_10\n    \"TRIGGER_ATC\": 35,   # ATC_5\n    \"CHRONIC_1\":   40,   # ICD_1\n    \"CHRONIC_2\":   52,   # ICD_2\n    \"CHRONIC_3\":   63,   # ICD_3\n}\n\n# Load feature names (from any scenario \u2014 same across all)\nfeat_csv = os.path.join(sc_dir(\"Baseline\"), \"feature_names_N3000_V20_trig0.15_esc0.10_bg0.010.csv\")\nif os.path.exists(feat_csv):\n    feat_df       = pd.read_csv(feat_csv)\n    feature_names = feat_df[\"name\"].tolist()\n    print(f\"Feature names loaded: {len(feature_names)} features\")\n    print(f\"  GT tokens: { {k: feature_names[v] for k,v in GT.items()} }\")\nelse:\n    feature_names = None\n    print(\"WARNING: feature_names CSV not found \u2014 run pipeline first or set manually\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n## 2. Load Faithfulness Results\n\nReads the `faithfulness.csv` saved by the pipeline. If a scenario hasn't been run yet it is skipped with a warning."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "faith_rows = []\nmissing    = []\n\nfor name in SCENARIOS:\n    csv = os.path.join(sc_dir(name), \"faithfulness.csv\")\n    if os.path.exists(csv):\n        df = pd.read_csv(csv)\n        df.insert(0, \"scenario\", name)\n        faith_rows.append(df)\n    else:\n        missing.append(name)\n\nif missing:\n    print(f\"WARNING \u2014 faithfulness.csv not found for: {missing}\")\n    print(\"Re-run the pipeline for those scenarios.\")\n\nif faith_rows:\n    faith = pd.concat(faith_rows, ignore_index=True)\n    print(f\"Loaded {len(faith)} rows across {faith['scenario'].nunique()} scenarios\")\n    display(faith.head(12))\nelse:\n    print(\"No results found. Run the pipeline first.\")\n    faith = pd.DataFrame()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n## 3. Overview \u2014 Metric \u00d7 Scenario \u00d7 Method\n\nOne heatmap per metric. Rows = scenarios, columns = method \u00d7 phenotype."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if not faith.empty:\n    metrics = faith[\"metric\"].unique()\n    phenotypes = [\"trigger\", \"escalation\", \"chronic\"]\n\n    for metric in metrics:\n        sub = faith[faith[\"metric\"] == metric]\n        pivot = sub.pivot_table(\n            index=\"scenario\", columns=[\"method\",\"phenotype\"], values=\"value\"\n        )\n        # Reorder rows to match SCENARIOS list\n        pivot = pivot.reindex([s for s in SCENARIOS if s in pivot.index])\n        # Only keep phenotypes of interest\n        cols_keep = [c for c in pivot.columns if c[1] in phenotypes]\n        pivot = pivot[cols_keep]\n\n        fig, ax = plt.subplots(figsize=(max(14, len(pivot.columns)*1.4), 5))\n        vmin = 0 if \"precision\" in metric or \"hit\" in metric else -0.1\n        vmax = 1.0\n        sns.heatmap(pivot, annot=True, fmt=\".3f\", cmap=\"YlGn\",\n                    vmin=vmin, vmax=vmax, ax=ax,\n                    linewidths=0.3, linecolor=\"#ddd\")\n        ax.set_title(f\"Faithfulness: {metric}\", fontsize=14, fontweight=\"bold\")\n        ax.set_xlabel(\"\")\n        ax.set_ylabel(\"\")\n        ax.set_xticklabels(\n            [f\"{m}\\n{p}\" for m, p in pivot.columns], rotation=30, ha=\"right\", fontsize=9\n        )\n        plt.tight_layout()\n        plt.subplots_adjust(top=0.88)\n        plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n## 4. Deep Dive by Phenotype\n\nOne subplot per metric, one line per method, x-axis = scenarios."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if not faith.empty:\n    phenotypes  = [\"trigger\", \"escalation\", \"chronic\"]\n    phen_titles = {\n        \"trigger\":    \"Trigger (ICD_10 \u2192 ATC_5, visits 1\u201310, weight 10)\",\n        \"escalation\": \"Escalation (broad ICD spike, last 3 visits, weight 2.5)\",\n        \"chronic\":    \"Chronic (ICD_1/2/3, all visits, weight 0.25)\",\n    }\n    metrics     = faith[\"metric\"].unique()\n    colors      = plt.cm.tab10.colors\n\n    for phen in phenotypes:\n        sub = faith[faith[\"phenotype\"] == phen]\n        if sub.empty:\n            continue\n\n        fig, axes = plt.subplots(1, len(metrics), figsize=(6*len(metrics), 5), sharey=False)\n        if len(metrics) == 1:\n            axes = [axes]\n\n        for ax, metric in zip(axes, metrics):\n            m_sub = sub[sub[\"metric\"] == metric]\n            for ci, method in enumerate(m_sub[\"method\"].unique()):\n                row = m_sub[m_sub[\"method\"] == method].set_index(\"scenario\")\n                vals = [row.loc[s, \"value\"] if s in row.index else np.nan\n                        for s in SCENARIOS]\n                ax.plot(range(len(SCENARIOS)), vals,\n                        marker=\"o\", label=method,\n                        color=colors[ci % len(colors)])\n\n            ax.set_xticks(range(len(SCENARIOS)))\n            ax.set_xticklabels(SCENARIOS, rotation=40, ha=\"right\", fontsize=8)\n            ax.set_ylabel(metric)\n            ax.set_title(metric, fontsize=11)\n            ax.set_ylim(-0.05, 1.05)\n            ax.axhline(0.5, color=\"gray\", linestyle=\"--\", linewidth=0.8, alpha=0.5)\n            ax.legend(fontsize=8)\n\n        fig.suptitle(phen_titles[phen], fontsize=13, fontweight=\"bold\")\n        plt.tight_layout()\n        plt.subplots_adjust(top=0.88)\n        plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n## 5. EG vs IG Head-to-Head\n\nDirect comparison of the two XAI methods for each model, averaged across scenarios. Shows which method is more faithful and whether the answer changes by phenotype or metric."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if not faith.empty:\n    for model in MODELS:\n        eg_col = f\"EG_{model}\"\n        ig_col = f\"IG_{model}\"\n        methods_avail = faith[\"method\"].unique()\n        if eg_col not in methods_avail or ig_col not in methods_avail:\n            print(f\"  {model}: EG or IG results missing, skipping\")\n            continue\n\n        sub = faith[faith[\"method\"].isin([eg_col, ig_col])]\n        agg = (sub.groupby([\"method\",\"phenotype\",\"metric\"])[\"value\"]\n                  .mean().reset_index()\n                  .pivot_table(index=[\"phenotype\",\"metric\"],\n                               columns=\"method\", values=\"value\")\n                  .round(3))\n        agg[\"EG_wins\"] = agg[eg_col] > agg[ig_col]\n        agg[\"delta\"]   = (agg[eg_col] - agg[ig_col]).round(3)\n\n        print(f\"\\n\u2500\u2500 {model}: EG vs IG (mean across scenarios) \u2500\u2500\")\n        display(agg.style\n                .background_gradient(subset=[eg_col, ig_col], cmap=\"YlGn\", axis=None)\n                .applymap(lambda v: \"color: #1a7a4a; font-weight:bold\" if v else\n                                    \"color: #b03030\",\n                          subset=[\"EG_wins\"]))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n## 6. Degradation Analysis\n\nHow does faithfulness degrade as the simulation becomes harder? Compares Baseline against the stress scenarios for each metric and phenotype.\n\n**Scenario groups:**\n- **Sample size**: Small N \u2192 Baseline \u2192 Large N\n- **Trigger rarity**: Very Rare \u2192 Rare \u2192 Baseline\n- **History length**: Short \u2192 Baseline \u2192 Long\n- **Noise level**: Baseline \u2192 High \u2192 Very High"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if not faith.empty:\n    groups = {\n        \"Sample size\":     [\"Small N\",          \"Baseline\",    \"Large N\"],\n        \"Trigger rarity\":  [\"Very Rare Trigger\", \"Rare Trigger\",\"Baseline\"],\n        \"History length\":  [\"Short History\",     \"Baseline\",    \"Long History\"],\n        \"Noise level\":     [\"Baseline\",          \"High Noise\",  \"Very High Noise\"],\n    }\n    phenotypes = [\"trigger\", \"escalation\", \"chronic\"]\n\n    # Use EG_GRU as the reference method; change here if needed\n    ref_method = \"EG_GRU\"\n\n    sub = faith[(faith[\"method\"] == ref_method) & (faith[\"phenotype\"].isin(phenotypes))]\n\n    fig, axes = plt.subplots(len(groups), len(phenotypes),\n                              figsize=(5*len(phenotypes), 4*len(groups)),\n                              sharey=False)\n\n    for gi, (group_name, group_scenarios) in enumerate(groups.items()):\n        for pi, phen in enumerate(phenotypes):\n            ax = axes[gi][pi]\n            g_sub = sub[(sub[\"phenotype\"] == phen) &\n                        (sub[\"scenario\"].isin(group_scenarios))]\n            for metric in g_sub[\"metric\"].unique():\n                m_sub = g_sub[g_sub[\"metric\"] == metric].set_index(\"scenario\")\n                vals  = [m_sub.loc[s, \"value\"] if s in m_sub.index else np.nan\n                         for s in group_scenarios]\n                ax.plot(range(len(group_scenarios)), vals,\n                        marker=\"o\", label=metric)\n            ax.set_xticks(range(len(group_scenarios)))\n            ax.set_xticklabels(group_scenarios, rotation=20, ha=\"right\", fontsize=9)\n            ax.set_ylim(-0.05, 1.05)\n            ax.axhline(0.5, color=\"gray\", linestyle=\"--\", linewidth=0.8, alpha=0.5)\n            if pi == 0:\n                ax.set_ylabel(group_name, fontsize=10, fontweight=\"bold\")\n            if gi == 0:\n                ax.set_title(phen.capitalize(), fontsize=11, fontweight=\"bold\")\n            ax.legend(fontsize=7)\n\n    fig.suptitle(f\"Faithfulness degradation by scenario group ({ref_method})\",\n                 fontsize=14, fontweight=\"bold\")\n    plt.tight_layout()\n    plt.subplots_adjust(top=0.95)\n    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n## 7. Feature Attribution Spotlight\n\nDirectly visualises whether the GT features (ICD_10, ATC_5, ICD_1/2/3) are attributed higher importance than background features. Loads `eg_*.npy` files directly."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "gt_feature_map = {\n    \"ICD_10 (trigger diag)\": 41,\n    \"ATC_5  (trigger med)\":  35,\n    \"ICD_1  (chronic)\":      40,\n    \"ICD_2  (chronic)\":      52,\n    \"ICD_3  (chronic)\":      63,\n}\nBACKGROUND_SAMPLE = 20   # random background features to compare against\n\nfor model in MODELS:\n    rows_data = []\n    for name in SCENARIOS:\n        npy = os.path.join(sc_dir(name), f\"eg_{model.lower()}.npy\")\n        if not os.path.exists(npy):\n            continue\n        eg = np.load(npy)   # (N_test, T, F)\n        mean_abs = np.mean(np.abs(eg), axis=(0, 1))   # (F,)\n\n        for label, idx in gt_feature_map.items():\n            rows_data.append({\"scenario\": name, \"feature\": label,\n                               \"type\": \"GT\", \"mean_abs_eg\": mean_abs[idx]})\n\n        # Sample background (non-GT) features\n        np.random.seed(42)\n        bg_idx = np.random.choice(\n            [i for i in range(140) if i not in gt_feature_map.values()],\n            BACKGROUND_SAMPLE, replace=False\n        )\n        rows_data.append({\"scenario\": name, \"feature\": \"Background (mean)\",\n                           \"type\": \"BG\",\n                           \"mean_abs_eg\": mean_abs[bg_idx].mean()})\n\n    if not rows_data:\n        print(f\"{model}: no .npy files found\")\n        continue\n\n    df_spot = pd.DataFrame(rows_data)\n    pivot_spot = df_spot.pivot_table(\n        index=\"scenario\", columns=\"feature\", values=\"mean_abs_eg\"\n    ).reindex([s for s in SCENARIOS if s in df_spot[\"scenario\"].values])\n\n    fig, ax = plt.subplots(figsize=(14, 5))\n    x = np.arange(len(pivot_spot))\n    ncols = len(pivot_spot.columns)\n    width = 0.8 / ncols\n    for ci, col in enumerate(pivot_spot.columns):\n        color = \"#e94560\" if \"BG\" not in col else \"#aaaaaa\"\n        ax.bar(x + ci*width, pivot_spot[col], width=width,\n               label=col, color=color, alpha=0.85)\n    ax.set_xticks(x + width*ncols/2)\n    ax.set_xticklabels(pivot_spot.index, rotation=30, ha=\"right\")\n    ax.set_ylabel(\"Mean |EG|\")\n    ax.set_title(f\"{model} \u2014 GT vs Background feature attribution across scenarios\",\n                 fontsize=13, fontweight=\"bold\")\n    ax.legend(fontsize=8, ncol=3)\n    plt.tight_layout()\n    plt.subplots_adjust(top=0.90)\n    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n## 8. Summary Table\n\nSingle table of all faithfulness results \u2014 export-ready for a paper."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if not faith.empty:\n    summary = (faith\n               .groupby([\"scenario\", \"method\", \"phenotype\", \"metric\"])[\"value\"]\n               .mean()\n               .reset_index()\n               .pivot_table(index=[\"scenario\",\"phenotype\"],\n                            columns=[\"method\",\"metric\"],\n                            values=\"value\")\n               .round(3))\n    summary = summary.reindex([s for s in SCENARIOS if s in summary.index.get_level_values(0)])\n    summary.to_csv(os.path.join(OUT_DIR, \"faithfulness_summary.csv\"))\n    print(f\"Saved \u2192 {os.path.join(OUT_DIR, 'faithfulness_summary.csv')}\")\n    display(summary)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ]
}