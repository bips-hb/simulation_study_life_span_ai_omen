{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EHR Prediction & XAI Benchmark \u2014 Multi-Scenario Pipeline\n",
    "\n",
    "Runs the full pipeline (data loading \u2192 classic models \u2192 DL training \u2192 IG \u2192 EG \u2192 counterfactuals) **once per simulation scenario** defined in **Cell 1**.\n",
    "\n",
    "All results land in `<OUT_DIR>/<scenario_tag>/` so every scenario stays isolated.\n",
    "\n",
    "**Sections**\n",
    "1. Configuration\n",
    "2. Scenario loop \u2014 everything below runs automatically for each scenario\n",
    "   - 2.1 Data loading & splitting\n",
    "   - 2.2 Classic models (LR, RF)\n",
    "   - 2.3 Deep learning models (GRU, LSTM)\n",
    "   - 2.4 Benchmark table\n",
    "   - 2.5 Case-study selection\n",
    "   - 2.6 XAI \u2014 Integrated Gradients (IG)\n",
    "   - 2.7 XAI \u2014 Expected Gradients (EG)\n",
    "   - 2.8 Counterfactual explanations\n",
    "3. Cross-scenario comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Configuration\n",
    "\n",
    "> **This is the only cell you need to edit.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport utils\n\nutils.set_seed(42)\n\n# \u2500\u2500 Paths \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nSIM_DIR = \"/mnt/c/Users/hoepler/Documents/LifeSpanAI_OMEN/simulation_study_life_span_ai_omen/sim_outputs\"   # folder where R saved the CSVs\nOUT_DIR = \"/home/hoepler/longitudinal_prediction_modeling/pipeline_out\"  # one sub-folder per scenario is created here\nos.makedirs(OUT_DIR, exist_ok=True)\n\n# \u2500\u2500 Scenario registry \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# Each entry maps a human-readable name to the scenario_tag produced by R\n# (the tag is printed by R as \"Files saved with tag: <tag>\") and the\n# N_patients / N_visits that were used so load_data knows the tensor dimensions.\n#\n# Format:\n#   \"display_name\": {\n#       \"tag\"       : \"<exact tag string from R>\",\n#       \"n_patients\": <int>,\n#       \"n_visits\"  : <int>,\n#   }\n\nSCENARIOS = {\n    \"Baseline\": {\n        \"tag\": \"N3000_V20_trig0.15_esc0.10_bg0.010\",\n        \"n_patients\": 3000,\n        \"n_visits\": 20,\n    },\n    \"Small N\": {\n        \"tag\": \"N500_V20_trig0.15_esc0.10_bg0.010\",\n        \"n_patients\": 500,\n        \"n_visits\": 20,\n    },\n    \"Large N\": {\n        \"tag\": \"N10000_V20_trig0.15_esc0.10_bg0.010\",\n        \"n_patients\": 10000,\n        \"n_visits\": 20,\n    },\n    \"Rare Trigger\": {\n        \"tag\": \"N3000_V20_trig0.05_esc0.10_bg0.010\",\n        \"n_patients\": 3000,\n        \"n_visits\": 20,\n    },\n    \"Very Rare Trigger\": {\n        \"tag\": \"N3000_V20_trig0.02_esc0.10_bg0.010\",\n        \"n_patients\": 3000,\n        \"n_visits\": 20,\n    },\n    \"Short History\": {\n        \"tag\": \"N3000_V10_trig0.15_esc0.10_bg0.010\",\n        \"n_patients\": 3000,\n        \"n_visits\": 10,\n    },\n    \"Long History\": {\n        \"tag\": \"N3000_V50_trig0.15_esc0.10_bg0.010\",\n        \"n_patients\": 3000,\n        \"n_visits\": 50,\n    },\n    \"High Noise\": {\n        \"tag\": \"N3000_V20_trig0.15_esc0.10_bg0.050\",\n        \"n_patients\": 3000,\n        \"n_visits\": 20,\n    },\n    \"Very High Noise\": {\n        \"tag\": \"N3000_V20_trig0.15_esc0.10_bg0.100\",\n        \"n_patients\": 3000,\n        \"n_visits\": 20,\n    },\n    \"Stress Test\": {\n        \"tag\": \"N5000_V30_trig0.05_esc0.10_bg0.050\",\n        \"n_patients\": 5000,\n        \"n_visits\": 30,\n    },\n}\n\n# \u2500\u2500 Model registry (shared across all scenarios) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nMODEL_REGISTRY = {\n    \"GRU\" : lambda n: utils.GRUModel( input_size=n, hidden_size=32, output_size=1, dropout_rate=0.5),\n    \"LSTM\": lambda n: utils.LSTMModel(input_size=n, hidden_size=32, output_size=1, dropout_rate=0.5),\n}\n\n# \u2500\u2500 Hyper-parameters (shared across all scenarios) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nTRAIN_CONFIG = dict(epochs=40, batch_size=32, lr=0.001)\nEG_CONFIG    = dict(background_size=128, nsamples=200, seed=42)\nIG_N_STEPS   = 64\nCF_CONFIG    = dict(threshold=0.5, max_edits=8, mode=\"token\", time_weight=0.0)\n\nPHENOTYPE_GROUPS = [\n    (\"trigger_only\",       \"Pure Trigger\"),\n    (\"chronic_only\",       \"Pure Chronic\"),\n    (\"escalation_only\",    \"Pure Late Escalation\"),\n    (\"trigger+chronic\",    \"Mixed: Trigger + Chronic\"),\n    (\"trigger+escalation\", \"Mixed: Trigger + Escalation\"),\n    (\"escalation+chronic\", \"Mixed: Chronic + Escalation\"),\n    (\"all_three\",          \"Mixed: All Three\"),\n]\n\n# Collector for cross-scenario comparison (populated in the loop)\nall_benchmarks    = {}\nall_faithfulness  = {}\n\nprint(f\"Scenarios registered: {list(SCENARIOS.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Scenario Loop\n",
    "\n",
    "The cell below runs the **entire pipeline** for every scenario in `SCENARIOS`.\n",
    "Each scenario writes its outputs to `OUT_DIR/<scenario_name>/`.\n",
    "\n",
    "To run a **single scenario** during development, pass a name to the filter:\n",
    "```python\n",
    "run_scenarios = [\"Baseline\"]   # \u2190 uncomment and set\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Optional: restrict to a subset during development\n# run_scenarios = [\"Baseline\", \"High Noise\"]\nrun_scenarios = list(SCENARIOS.keys())   # default: all\n\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nfor scenario_name in run_scenarios:\n    cfg = SCENARIOS[scenario_name]\n    tag = cfg[\"tag\"]\n\n    print(f\"\\n{'='*70}\")\n    print(f\"  SCENARIO: {scenario_name}  |  tag: {tag}\")\n    print(f\"{'='*70}\")\n\n    # Per-scenario output directory\n    sc_out = os.path.join(OUT_DIR, scenario_name.replace(\" \", \"_\"))\n    os.makedirs(sc_out, exist_ok=True)\n\n    # ------------------------------------------------------------------\n    # 2.1  Data loading & splitting\n    # ------------------------------------------------------------------\n    X, y, tokens, feature_names, labels = utils.load_data(\n        SIM_DIR,\n        n_patients=cfg[\"n_patients\"],\n        n_visits=cfg[\"n_visits\"],\n        scenario_tag=tag,\n    )\n\n    patient_ids = np.arange(1, cfg[\"n_patients\"] + 1)\n\n    (X_train, X_test,\n     y_train, y_test,\n     pid_train, pid_test,\n     labels_test) = utils.split_data(X, y, labels, patient_ids)\n\n    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n    X_train_np    = np.array(X_train)\n    X_test_np     = np.array(X_test)\n    np.save(os.path.join(sc_out, 'X_test.npy'), X_test_np)\n    utils.save_feature_names(feature_names, sc_out, scenario_tag=tag)\n\n    # ------------------------------------------------------------------\n    # 2.2  Classic models\n    # ------------------------------------------------------------------\n    X_train_flat = X_train.reshape(X_train.shape[0], -1)\n    X_test_flat  = X_test.reshape(X_test.shape[0],  -1)\n\n    lr_model = LogisticRegression(penalty=\"l2\", C=1.0, max_iter=1000)\n    lr_model.fit(X_train_flat, y_train)\n\n    rf_model = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=3105)\n    rf_model.fit(X_train_flat, y_train)\n\n    # ------------------------------------------------------------------\n    # 2.3  Deep learning models\n    # ------------------------------------------------------------------\n    trained_models = utils.train_all_models(\n        MODEL_REGISTRY, X_train, y_train, X_test, y_test,\n        train_config=TRAIN_CONFIG, out_dir=sc_out,\n    )\n\n    # ------------------------------------------------------------------\n    # 2.4  Benchmark table\n    # ------------------------------------------------------------------\n    bench = utils.benchmark_table(\n        trained_models, lr_model, rf_model,\n        X_train, X_test, y_train, y_test,\n    )\n    bench.insert(0, \"Scenario\", scenario_name)\n    bench.to_csv(os.path.join(sc_out, \"benchmark.csv\"), index=False)\n    all_benchmarks[scenario_name] = bench\n    print(\"\\nBenchmark:\")\n    display(bench.drop(columns=\"Scenario\"))\n\n    # ------------------------------------------------------------------\n    # 2.5  Case-study selection\n    # ------------------------------------------------------------------\n    ref_model = next(iter(trained_models.values()))\n    with torch.no_grad():\n        y_probs_ref = ref_model(X_test_tensor).numpy().ravel()\n\n    case_studies = utils.select_case_studies(\n        labels_test, y_test, y_probs_ref, PHENOTYPE_GROUPS\n    )\n\n    # ------------------------------------------------------------------\n    # 2.6  XAI \u2014 Integrated Gradients (IG)\n    # ------------------------------------------------------------------\n    print(\"\\n--- IG global ---\")\n    utils.run_ig_global(\n        trained_models, X_test_tensor, y_test,\n        feature_names, n_steps=IG_N_STEPS, out_dir=sc_out,\n    )\n\n    print(\"\\n--- IG case studies ---\")\n    utils.run_ig_case_studies(\n        trained_models, X_test_tensor, y_test,\n        feature_names, case_studies,\n        n_steps=IG_N_STEPS, out_dir=sc_out,\n    )\n\n    # ------------------------------------------------------------------\n    # 2.7  XAI \u2014 Expected Gradients (EG)\n    # ------------------------------------------------------------------\n    print(\"\\n--- EG (all models) ---\")\n    eg_by_model, base_by_model = utils.run_eg_all_models(\n        trained_models, X_train_np, X_test_np,\n        eg_config=EG_CONFIG, out_dir=sc_out,\n    )\n\n    utils.run_eg_global_plots(eg_by_model, X_test_np, feature_names, out_dir=sc_out)\n\n    utils.run_eg_local_plots(\n        eg_by_model, base_by_model, X_test_np,\n        feature_names, patient_idx=2, out_dir=sc_out,\n    )\n\n    utils.run_eg_case_studies(\n        eg_by_model, feature_names, case_studies,\n        trained_models, X_test_tensor, y_test, out_dir=sc_out,\n    )\n\n    # ------------------------------------------------------------------\n    # 2.8  Counterfactual explanations\n    # ------------------------------------------------------------------\n    print(\"\\n--- Counterfactuals (all timesteps) ---\")\n    cf_results = utils.run_cf_all_models(\n        trained_models, X_test, y_test, pid_test,\n        feature_names, tokens, case_studies,\n        cf_config=CF_CONFIG, out_dir=sc_out,\n    )\n\n    print(\"\\n--- Counterfactuals (escalation window only) ---\")\n    T_cf = X_test.shape[1]\n    cf_results_esc = utils.run_cf_all_models(\n        trained_models, X_test, y_test, pid_test,\n        feature_names, tokens, case_studies,\n        cf_config={**CF_CONFIG, \"allowed_timesteps\": list(range(T_cf - 3, T_cf))},\n        out_dir=sc_out,\n    )\n\n    # \u2500\u2500 2.9  XAI faithfulness evaluation \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    print(\"\\n--- Faithfulness evaluation ---\")\n    truth_ntf = utils.load_truth_arrays(\n        SIM_DIR, tag,\n        n_patients=cfg['n_patients'],\n        n_visits=cfg['n_visits'],\n        feature_names=feature_names,\n    )\n    # test_indices: position of each test patient in the full cohort (0-based)\n    test_indices = np.array([int(p) - 1 for p in pid_test])\n    faith_df = utils.run_faithfulness_evaluation(\n        eg_by_model=eg_by_model,\n        ig_global_maps=ig_global_maps,\n        truth_ntf=truth_ntf,\n        test_indices=test_indices,\n        phenotype_labels=labels_test,\n        n_visits=cfg['n_visits'],\n        out_dir=sc_out,\n    )\n    all_faithfulness[scenario_name] = faith_df\n    display(faith_df.pivot_table(\n        index=['phenotype','metric'], columns='method', values='value'\n    ).round(3))\n\n    print(f\"\\n\u2713  Scenario '{scenario_name}' complete \u2192 {sc_out}\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"ALL SCENARIOS COMPLETE\")\nprint(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Cross-Scenario Comparison\n",
    "\n",
    "Stacks all per-scenario benchmark tables into one view so you can compare how model performance changes across simulation conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack all benchmark tables\n",
    "comparison = pd.concat(all_benchmarks.values(), ignore_index=True)\n",
    "comparison.to_csv(os.path.join(OUT_DIR, \"benchmark_all_scenarios.csv\"), index=False)\n",
    "\n",
    "# Pivot: one row per scenario, columns = Model \u00d7 metric\n",
    "pivot = comparison.pivot_table(\n",
    "    index=\"Scenario\", columns=\"Model\", values=\"Test AUC\"\n",
    ").round(4)\n",
    "pivot.columns.name = None\n",
    "pivot = pivot.reindex(list(SCENARIOS.keys()))   # preserve scenario order\n",
    "\n",
    "print(\"Test AUC by scenario and model:\")\n",
    "display(pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "pivot.plot(kind=\"bar\", ax=ax, rot=30)\n",
    "ax.set_ylabel(\"Test AUC\")\n",
    "ax.set_title(\"Model performance across simulation scenarios\")\n",
    "ax.legend(title=\"Model\", bbox_to_anchor=(1.01, 1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"benchmark_comparison.png\"), dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}